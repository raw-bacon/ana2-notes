\documentclass[../main.tex]{subfiles}

\begin{document}
\chapter{Gewöhnliche Differentialgleichungen}\label{chp:odes}

\section{Kurven und Vektorfelder}
\begin{definition}
  Eine \emph{Kurve} in $\mathbb{R}^n$
  ist das Bild einer stetigen Abbildung
  $\gamma \colon \mathbb{R} \to \mathbb{R}^n$.
  Die Abbildung $\gamma$ heisst
  \emph{Parametrisierung} der Kurve $\gamma$.
  Wir werden die Unterscheidung zwischen Kurven
  und Parametrisierungen nicht immer explizit
  machen.
\end{definition}

Wir untersuchen nun kurz den Stetigkeitsbegriff auf Kurven.
Stetigkeit heisst, dass in jedem $t \in \mathbb{R}$ die
$(\varepsilon, \delta)$-Bedingung für $\gamma$
bezüglich beliebiger Normen auf $\mathbb{R}$ und $\mathbb{R}^n$
erfüllt ist, insbesondere
bezüglich der Normen $| \cdot |$ und $\Vert \cdot \Vert_{\infty}$.
Schreibe nun
\[
  \gamma(t) = \sum_{i=1}^{n} \gamma_i(t) \cdot e_i,
\]
wobei $e_i$ der $i$-te Basisvektor in $\mathbb{R}^n$ ist.
In diesem Fall ist $\gamma_i(t) = \langle \gamma(t), e_i \rangle$  % für Standardskalarprodukt; nicht für SkP allgemein, oder?
die $i$-te Komponentenfunktion von $\gamma$.
Für alle $s, t \in \mathbb{R}$ gilt
$\Vert \gamma(s) - \gamma(t) \Vert_\infty \leq \varepsilon$
genau dann, wenn $|\gamma_i(s) - \gamma_i(t)| \leq \varepsilon$
für alle  $i$.
Wir folgern, dass $\gamma$ stetig in $t \in \mathbb{R}$
ist, genau dann wenn alle Komponentenfunktionen
$\gamma_i$ stetig in $t$ sind.

\begin{example}
  \leavevmode
  \begin{enumerate}[(1)]
    \item
      Betrachte die Funktion
      \begin{align*}
        \gamma \colon \mathbb{R} & \to \mathbb{R}^2 \\
        t & \mapsto (\cos(t), \sin(t)).
      \end{align*}
      Dann hat $\gamma$ stetige
      Komponentenfunktionen und ist
      daher die Parametrisierung einer Kurve.
      In diesem Fall ist die beschriebene Kurve
      der Einheitskreis, siehe Abbildung~\ref{fig:parametrisations}.
      Die Pfeile erklären wir später.
    \item Die Funktion
      \begin{align*}
        \gamma \colon \mathbb{R} & \to \mathbb{R}^2 \\
        t & \mapsto (t^2, t^3 - t)
      \end{align*}
      ist stetig und somit die Parametrisierung einer Kurve,
      dargestellt in Abbildung~\ref{fig:parametrisations}.
      Diese Kurve hat einen ``singulären Punkt'',
      herbeigeführt durch die Eigenschaft,
      dass $\gamma(1) = \gamma(-1) = 1$ gilt.
  \end{enumerate}
\end{example}

\begin{figure}[htb]
  \centering
  \begin{minipage}{0.50\textwidth}
    \centering
    \includegraphics{figures/circlecurve}
  \end{minipage}%
  \begin{minipage}{0.50\textwidth}
    \centering
    \includegraphics{figures/butterflycurve}
  \end{minipage}%
  \caption{Die Bilder der Kurven $\gamma$
  aus den Beispielen}%
  \label{fig:parametrisations}
\end{figure}

\begin{definition}
  Eine parametrisierte Kurve $\gamma \colon \mathbb{R} \to \mathbb{R}^n$
  heisst \emph{differenzierbar}, falls für alle $t \in \mathbb{R}$
  die Ableitung
  \[
    \dot{\gamma}(t) = \lim_{h \to 0} \frac{ \gamma(t + h) - \gamma(t)}{h}
  \]
  existiert.
  Der Vektor $\dot{\gamma}(t)$ heisst \emph{Geschwindigkeitsvektor}.
\end{definition}

\begin{remark}
  Für alle $t, h \in \mathbb{R}$ mit $h \neq 0$
  gilt
  \[
    \frac{\gamma(t + h) - \gamma(t)}{h}
    = \sum_{k=1}^{n} \frac{\gamma_k(t + h)- \gamma_k(t)}{h} \cdot e_k
  \]
  Wir folgern, dass $\dot{\gamma}(t) \in \mathbb{R}^n$ existiert,
  genau dann, wenn alle Ableitungen $\dot{\gamma}_k(t) \in \mathbb{R}$
  existieren.
  In anderen Worten: Eine Kurve $\gamma \colon \mathbb{R} \to \mathbb{R}^n$
  ist differenzierbar, genau dann, wenn alle $\gamma_k \colon \mathbb{R}
  \to \mathbb{R}$ differenzierbar sind.
\end{remark}

\begin{examples}
  \leavevmode
  \begin{enumerate}[(1)]
    \item Die Kurve
      \begin{align*}
        \gamma \colon \mathbb{R} & \to \mathbb{R}^2 \\
        t & \mapsto (\cos(t), \sin(t))
      \end{align*}
      ist differenzierbar mit $\dot{\gamma}(t) = (-\sin(t), \cos(t))$.
      Die grauen Pfeile in Abbildung~\ref{fig:parametrisations}
      stellen die Ableitung von $\gamma$ dar.
    \item Die Kurve
      \begin{align*}
        \gamma \colon \mathbb{R} & \to \mathbb{R}^2 \\
        t & \mapsto (t^2, t^3 - t)
      \end{align*}
      ist differenzierbar mit $\dot{\gamma}(t) = (2t, 3t^2 - 1)$.
      Wir berechnen
      \begin{itemize}
        \item $\gamma(0) = (0, 0)$ und $\dot{\gamma}(0) = (0, -1)$,
        \item $\gamma(1) = (1, 0)$ und $\dot{\gamma}(1) = (2, 2)$,
        \item $\gamma(-1) = (1, 0)$ und $\dot{\gamma}(-1) = (-2, 2)$,
        \item $\gamma(1/\sqrt{3}) = (1/3, -2/3 \cdot 1/\sqrt{3})$
          und $\dot \gamma (1/\sqrt{3}) = (2 / \sqrt 3, 0)$,
        \item $\gamma(-1 / \sqrt 3) = (1 / 3, 2/ 3 \cdot 1 / \sqrt 3)$
          und $\dot \gamma ( - 1 / \sqrt 3 ) = (- 2 / \sqrt 3, 0)$.
      \end{itemize}
     Für grosse $t$ ist $y$ ungefähr $x^{3/2}$. Diese Information reicht,
     um das Bild von $\gamma$ wie in Abbildung~\ref{fig:parametrisations}
     zu erraten.
     Die Ableitungen sind in grau (mit $1/4$ gestreckt) eingezeichnet.
  \end{enumerate}
\end{examples}

\begin{definition}
  Sei $U \subset \mathbb{R}^n$ offen.
  Ein \emph{Vektorfeld} auf $U$ ist eine stetige Abbildung
  \begin{align*}
    X \colon U & \to \mathbb{R}^n \\
    q & \mapsto X(q).
  \end{align*}
  Jedes Vektorfeld $X$ auf $U$ liefert eine
  \emph{gewöhnliche Differentialgleichung}
  $\dot \gamma(t) = X(\gamma(t))$.
  Dies ist wie folgt zu interpretieren.
  Sei $p \in U$ fest. Gesucht ist $T > 0$
  und eine differenzierbare  \emph{Lösungskurve}
  $\gamma  \colon (-T, T) \to U$
  zur Anfangsbedingung $\gamma(0) = p$.
  Falls $X = A \colon \mathbb{R}^n \to \mathbb{R}^n$
  linear ist, dann heisst die Differentialgleichung
  $\dot \gamma(t) = A(\gamma(t))$
  \emph{linear}.
\end{definition}

\begin{geometric}
  Ein Vektorfeld $X \colon U \to \mathbb{R}^2$
  liefert in jedem Punkt $q \in U$ einen Vektor
  $X(q) \in \mathbb{R}^n$.
  Siehe Abbildung~\ref{fig:vectorfield}.
  In jedem Punkt der Kurve
  $\gamma(t) \in U$ stimmt der Geschwindigkeitsvektor
  $\dot \gamma(t) \in \mathbb{R}^n$
  mit dem Vektor $X(\gamma(t))$ überein.
\end{geometric}

\begin{figure}[ht]
    \centering
    \incfig{vectorfield}
    \caption{Ein Vektorfeld $X$ auf $U \subset \mathbb{R}^2$
    mit einer Lösungskurve $\gamma$ in schwarz}%
    \label{fig:vectorfield}
\end{figure}

\newpage

\begin{examples}
  \leavevmode
  \begin{enumerate}[(1)]
    \item Betrachte das lineare Vektorfeld
      \begin{align*}
        X \colon \mathbb{R} & \to \mathbb{R}^n \\
        q & \mapsto 0.
      \end{align*}
      Sei $p \in \mathbb{R}^n$. Dann ist $\gamma(t) = p$
      die eindeutige
      Lösung der Differentialgleichung $\gamma(0) = p$
      und $\dot \gamma(t) = X(\gamma(t))$.
    \item Betrachte das lineare Vektorfeld
      \begin{align*}
        A \colon \mathbb{R}^2 & \to \mathbb{R}^2 \\
        \begin{pmatrix}
          x \\ y
        \end{pmatrix}
         & \mapsto
         \begin{pmatrix}
           0 & -1 \\ 1 & 0
         \end{pmatrix}
         \begin{pmatrix}
           x \\ y
         \end{pmatrix}
         =
         \begin{pmatrix}
           -y \\ x
         \end{pmatrix},
      \end{align*}
      siehe Abbildung~\ref{fig:vectorfields-examples}.
      Betrachte nochmals die Kurve
      \begin{align*}
        \gamma \colon \mathbb{R} & \to \mathbb{R}^2 \\
        t & \mapsto
        \begin{pmatrix}
          \cos(t) \\ \sin(t)
        \end{pmatrix}.
      \end{align*}
      Es gilt
      \[
        \dot \gamma(t) =
        \begin{pmatrix}
          - \sin(t) \\ \cos(t)
        \end{pmatrix}
        =
        \begin{pmatrix}
          0 & -1 \\ 1 & 0
        \end{pmatrix}
        \begin{pmatrix}
          \cos(t) \\ \sin(t)
        \end{pmatrix}
        = A(\gamma(t)).
      \]
      Also ist die Kurve $\gamma \colon \mathbb{R} \to \mathbb{R}^2$
      eine Lösung der linearen Differentialgleichung
      $\dot \gamma(t) = A(\gamma(t))$
      zur Anfangsbedingung $\gamma(0) = e_1 \in \mathbb{R}^2$.
      Allgemeiner ist die Funktion
      \begin{align*}
        \gamma \colon \mathbb{R} & \to \mathbb{R}^2 \\
        t & \mapsto
        \begin{pmatrix}
          r \cos(t + \varphi) \\
          r \sin(t + \varphi)
        \end{pmatrix}
      \end{align*}
      eine Lösung dieser Differentialgleichung.
      Durch geschickte Wahl von $\varphi$ und $r$
      deckt das jede Anfangsbedingung ab.
      \begin{figure}[htb]
        \centering
        \begin{minipage}{0.50\textwidth}
          \centering
          \includegraphics{figures/rotationfield}
        \end{minipage}%
        \begin{minipage}{0.50\textwidth}
          \centering
          \includegraphics{figures/yzero}
        \end{minipage}%
        \caption{Das Vektorfeld $A(x, y) = (-y, x)$ und das
        Vektorfeld $A(x, y) = (y, 0)$}% ev. noch auf die Bspl. (2 bzw. 4) verweisen :)
        \label{fig:vectorfields-examples}
      \end{figure}
    \item Die Kurve $\gamma(t) = (t^2, t^3 - t)$ ist
      nicht die Lösung einer gewöhnlichen Differentialgleichung.
      Der Grund dafür ist, dass $\gamma$ den Punkt
      $(1, 0)$ zweimal mit unterschiedlichen Geschwindigkeitsvektoren
      durchläuft: Es gilt $\gamma(1) = \gamma(-1)$.
      aber $\dot \gamma(1) \neq \dot \gamma(-1)$.
      Falls nämlich $X \colon \mathbb{R}^2 \to \mathbb{R}^2$
      existiert mit $\dot \gamma (t) = X ( \gamma(t))$,
      dann gälte
      \begin{align*}
        X(e_1) &= X(\gamma(1)) = \dot \gamma(1) = (2, 2),\\
        X(e_1) &= X(\gamma(-1)) = \dot \gamma(-1) = (-2, 2).
      \end{align*}
      Das ist aber nicht möglich, da $2 \neq -2$.
    \item Betrachte das lineare Vektorfeld
      \begin{align*}
        A \colon \mathbb{R}^2 & \to \mathbb{R}^2 \\
        \begin{pmatrix}
          x \\ y
        \end{pmatrix}
         & \mapsto
         \begin{pmatrix}
           0 & 1 \\ 0 & 0
         \end{pmatrix}
         \begin{pmatrix}
           x \\ y
         \end{pmatrix}
         =
         \begin{pmatrix}
           y \\ 0
         \end{pmatrix}.
      \end{align*}
      Sei $p = (x_0, y_0)$ vorgegeben.
      Als Ansatz für $\gamma$
      schreiben wir $\gamma(t) = (x(t),
      y(t))$.
      Aus $\dot \gamma(t) = (y(t), 0)$ erhalten wir
      $\dot y (t) = 0$, also ist $y(t) = y_0$ konstant.
      Weiterhin ist
      $\dot x(t) = y(t) = y_0$, also ist
      $x(t) = y_0 t + x_0$. Zusammengefasst erhalten
      wir die Lösungskurve
      \[
        \gamma(t) =
        \begin{pmatrix}
          x_0 + y_0 t \\
          y_0
        \end{pmatrix}.
      \]
    \item Sei $A \colon \mathbb{R}^n \to \mathbb{R}^n$
      beliebig und linear.
      Sei $v \in \mathbb{R}^n$ ein Eigenvektor zum Eigenwert
      $\lambda \in \mathbb{R}$, das heisst
      $A(v) = \lambda v$.
      Definiere
       \begin{align*}
        \gamma \colon \mathbb{R} & \to \mathbb{R}^n \\
        t & \mapsto e^{\lambda t} \cdot v.
      \end{align*}
      Es gilt $\gamma(0) = v$ und
      \[
        \dot \gamma(t) = e^{\lambda t} \cdot \lambda \cdot v
        = e^{\lambda t}  \cdot A(v)
        = A(e^{\lambda t} \cdot v).
      \]
      Also ist $\gamma$ eine Lösungskurve.
      Wir haben aber verwendet, dass $v$ ein Eigenvektor
      von $A$ ist. Die anderen Lösungskurven von $A$
      könnten komplizierter sein.
    \item Sei $A \colon \mathbb{R}^n \to \mathbb{R}^n$
      linear und \emph{diagonalisierbar} über $\mathbb{R}$,
      das heisst, es existieren
      \emph{Eigenwerte} $\lambda_1, \dots, \lambda_n \in \mathbb{R}$
      und eine Basis aus \emph{Eigenvektoren}
      $\{v_1, \dots, v_n\}$ von $\mathbb{R}^n$
      mit $A(v_i) = \lambda_i v_i$.
      Sei $p \in \mathbb{R}^n$ vorgegeben. Schreibe
      \[
        p = \sum_{i=1}^{n} x_i v_i
      \]
      mit $x_i \in \mathbb{R}$.
      Definiere
      \begin{align*}
        \gamma \colon \mathbb{R} & \to \mathbb{R}^n \\
        t & \mapsto e^{\lambda_1 t} \cdot x_1 \cdot v_1 +
        \cdots
        +
        e^{\lambda_n t} \cdot x_n \cdot v_n.
      \end{align*}
      Es gilt $\gamma(0) = p$ und
      \[
        \dot \gamma(t) = \sum_{i=1}^{n} \lambda_i e^{\lambda_i t}x_i v_i
        = A(\gamma(t)).
      \]
      Es existiert also eine Lösungskurve $\gamma(t)$
      für beliebige Anfangsbedingungen.
    \item Sei
      \begin{align*}
        f \colon \mathbb{R} & \to \mathbb{R} \\
        x & \mapsto 2 \sqrt{|x|}
      \end{align*}
      ein Vektorfeld auf $\mathbb{R}$.
      Betrachte die Anfangsbedingung $p = 0$.
      Die konstante Nullkurve ist
      eine Lösungskurve.
      Betrachte nun die Kurve
      \begin{align*}
        \gamma \colon \mathbb{R} & \to \mathbb{R} \\
        t & \mapsto
        \begin{cases}
          0 & t \leq 0 \\
          t^2 & t \geq 0.
        \end{cases}
      \end{align*}
      Es gilt $\gamma(0) = 0$ und
      \[
        \dot \gamma(t) =
        \begin{cases}
          0 & t \leq 0 \\
          2t & t \geq 0.
        \end{cases}
      \]
      Es gilt aber auch
      \[
        f(\gamma(t)) =
        \begin{cases}
          2 \sqrt{|0|} = 0 & t \leq 0 \\
          2 \sqrt{|t^2|} = 2t & t \geq 0.
        \end{cases}
      \]
      Das heisst $\gamma(t)$ ist auch eine Lösung
      zur selben Anfangsbedingung.
      Das ``Problem'' hier ist,
      dass $f \colon \mathbb{R} \to \mathbb{R}$
      nicht Lipschitz-stetig ist.
  \end{enumerate}
\end{examples}

\section{Der Satz von Cauchy-Lipschitz-Picard-Lindelöf}
\begin{definition}
  Sei $U \subset \mathbb{R}^n$ offen.
  Eine Abbildung $X \colon U \to \mathbb{R}^n$
  heisst \emph{Lipschitz-stetig} mit Konstante
  $k \geq 0$, falls für alle $p, q \in U$
  die Ungleichung
  \[
    \Vert X(q) - X(p) \Vert_2 \leq k \cdot \Vert q - p \Vert_2
  \]
  gilt.
\end{definition}

\begin{remark}
  Die Wahl der Norm $\Vert \cdot \Vert_2$ ist für diese
  Definition irrelevant,
  da alle Normen auf $\mathbb{R}^n$ äquivalent sind.
  Die Konstante $k \geq 0$ hängt aber von der gewählten
  Norm ab.
\end{remark}

\begin{theorem*}[Cauchy-Lipschitz-Picard-Lindelöf]
  Sei $U \subset \mathbb{R}^n$ offen
  und $X \colon U \to \mathbb{R}^n$ Lipschitz-stetig
  mit Konstante $k \geq 0$.
  Sei $p \in U$ vorgegeben.
  Dann existiert $T > 0$ und eine \emph{eindeutige}
  differenzierbare Kurve
  $\gamma \colon (-T, T) \to U$ mit $\gamma(0) = p$,
  so dass für alle $t \in (-T, T)$ gilt, dass
  $\dot \gamma(t) = X(\gamma(t))$.
\end{theorem*}

\begin{remark}
  \leavevmode
  \begin{enumerate}[\normalfont1.]
    \item Falls $U = \mathbb{R}^n$ ist, kann $T > 0$
      beliebig gewählt werden.
      Siehe dazu Übungsserie 4.
    \item Falls $U = \mathbb{R}^n$ und $k = 0$,
      dann gilt für alle $p,q \in \mathbb{R}^n$,
      dass
      $\Vert X(p)  - X(q) \Vert_2
      \leq 0 \cdot \Vert p - q \Vert_2 = 0$.
      Das Vektorfeld $X$ ist also konstant.
      Dann ist die Lösungskurve
      \begin{align*}
        \gamma \colon \mathbb{R} & \to \mathbb{R}^n \\
        t & \mapsto p + t \cdot X(p).
      \end{align*}
  \end{enumerate}
\end{remark}


Vergleiche~\cite{heuser}, Satz 117.1.
Ein wichtiger Unterschied in der Version,
die Heuser formuliert, ist es,
dass das Vektorfeld $f$ dort auch von der Zeit $x$
abhängen darf.
Wir werden nun einige Vorbereitungen zum Beweis
treffen.
Dazu werden wir zunächst Kurven genauer betrachten.

\begin{definition}
  Sei $\alpha \colon [a, b] \to \mathbb{R}^n$ stetig.
  Schreibe
  \[\alpha(t) = \sum_{i=1}^{n} \alpha_i(t) e_i,
  \]
  wobei $\alpha_i \colon [a, b] \to \mathbb{R}$ stetige Funktionen sind.
  Wir schreiben dann
  \[
    \int_{a}^{b} \alpha(s) \, ds
    = \sum_{i=1}^{n} \left(
      \int_a^b
      \alpha_i(s) \, ds
    \right) \cdot e_i.
  \]
\end{definition}

\begin{proposition}
  Seien $\alpha \colon [a, b] \to \mathbb{R}^n$ stetig
  und $\gamma \colon (a, b) \to \mathbb{R}^n$
  stetig differenzierbar.
  Dann gilt:
  \begin{enumerate}[\normalfont(i)]
    \item für alle $t \in (a, b)$ gilt
      \[
        \frac{d}{dt} \int_{a}^{t} \alpha(s) \, ds = \alpha(t),
      \]
    \item unter der Annahme, dass $[0, t] \subset (a, b)$, gilt
      \[
        \gamma(t) - \gamma(0) = \int_{0}^{t} \dot \gamma(s) \, ds.
      \]
  \end{enumerate}
\end{proposition}

\begin{proof}
  Sowohl die Differentiation als auch die Integration von Kurven
  lässt sich komponentenweise ausführen.
  Deshalb folgen die beiden Eigenschaften aus den entsprechenden
  Sätzen der Differential- und Integralrechnung für Funktionen
  auf $\mathbb{R}$.
\end{proof}

Sei $C([a, b], \mathbb{R}^n)$ der Raum aller stetigen
Funktionen $\alpha \colon [a, b] \to \mathbb{R}^n$.
Für eine Funktion $\alpha \in C([a, b], \mathbb{R}^n)$ definieren wir
\[
  \Vert \alpha \Vert_{\infty}
  = \max \left\{\Vert \alpha(t) \Vert_2 \mid
  t \in [a, b]\right\} \geq 0.
\]

\begin{lemma}
  Der Raum $C([a, b], \mathbb{R}^n)$ ist bezüglich der Norm
  $\Vert \cdot \Vert_{\infty}$ vollständig.
\end{lemma}

\begin{proof}
  Sei ${(\alpha_{m})}_{m \in \mathbb{N}}$
  eine Cauchyfolge in $C([a, b], \mathbb{R}^n)$ bezüglich der Norm
  $\Vert \cdot \Vert_{\infty}$.
  Dann erhalten wir in jeder Komponente $i \leq n$
  eine Cauchyfolge $\langle \alpha_m, e_i \rangle$
  in $C([a, b], \mathbb{R}^1)$ (bezüglich der Norm
  $\Vert \cdot \Vert_{\infty}$ auf $C([a, b], \mathbb{R}^1)$).
  Nach Weierstrass existiert für alle $i \leq n$
  der Grenzwert
  $\lim_{m \to \infty} \langle \alpha_m, e_i \rangle$
  in $C([a, b], \mathbb{R})$.
  Es folgt
  wie im Beweis von Banachs Fixpunktsatz, dass der Grenzwert
  \[
    \lim_{m \to \infty} \alpha_m
    = \sum_{i=1}^{n} \lim_{m \to \infty}
    \langle \alpha_m, e_i \rangle \cdot e_i
  \]
  in $C([a, b], \mathbb{R}^n)$ existiert.
  Also ist $C([a, b], \mathbb{R}^n)$ mit der Norm $\Vert \cdot \Vert_\infty$ vollständig.
\end{proof}

% Die Beweisidee vom Theorem ist nun folgendermassen.
% Sei $\gamma \colon (-T, T) \to U$
% differenzierbar mit $\gamma(0) = p$
% und $\dot \gamma(t) = X(\gamma(t))$.
% Dann ist $\dot \gamma \colon (-T, T) \to \mathbb{R}^n$
% stetig, da $X$ stetig ist, und somit auch $X \circ \gamma$.
% Aus Punkt (ii) der Proposition folgt für alle
% $t \in (-T, T)$, dass
% \[
%   \gamma(t) - \gamma(0) = \int_{0}^{t} \dot \gamma(s) \, ds,
% \]
% beziehungsweise
% \[
%   \gamma(t) = p + \int_{0}^{t} X(\gamma(s)) \, ds.
% \]
% Wir definieren nun einen Operator
% \begin{align*}
%   P \colon C([-T, T], \mathbb{R}^n) & \to
%   C([-T, T], \mathbb{R}^n) \\
%   \alpha & \mapsto P(\alpha),
% \end{align*}
% wobei $P(\alpha)$ die Kurve
% \begin{align*}
%   P(\alpha) \colon [-T, T] & \to \mathbb{R}^n \\
%   t& \mapsto p + \int_{0}^{t} X(\alpha(s)) \, ds
% \end{align*}
% ist.
% Der Einfachheit halber nehmen wir zusätzlich an,
% dass $U = \mathbb{R}^n$.
% Es gilt:
% \begin{enumerate}[(i)]
%   \item Die Kurve $\gamma$ ist ein Fixpunkt von $P$, das heisst
%     $P(\gamma)(t) = \gamma(t)$ für alle $t$.
%   \item Sei umgekehrt $\gamma \in C([-T, T], \mathbb{R}^n)$
%     ein Fixpunkt von $P$.
%     Für alle $t \in (-T, T)$ gilt dann, dass
%     \[
%       \gamma(t) = p + \int_{0}^{t} X(\gamma(s)) \, ds,
%     \]
%     also ist $\gamma(0) = p$, und
%     $\dot \gamma(t) = X(\gamma(t))$.
% \end{enumerate}

Der Beweis des
Theorems reduziert sich auf ein Fixpunktproblem,
welches wir mit Banachs Fixpunktsatz lösen können.
Dazu verwenden wir folgende Abschätzung.

\begin{lemma}
  Sei $\alpha \colon [a, b] \to \mathbb{R}^n$ stetig
  und $t \in \mathbb{R}$ mit $[0, t] \subset [a, b]$.
  Dann gilt
  \[
    \left\Vert \int_{0}^{t} \alpha(s) \, ds \right\Vert_2
    \leq \int_{0}^{t} \Vert \alpha(s) \Vert_2 \, ds.
  \]
\end{lemma}

\begin{proof}
  Setze $v = \int_{0}^{t} \alpha(s) \, ds$
  Berechne
  \begin{align*}
    \Vert v \Vert_2^2
    &= \langle v, v \rangle\\
    &= \left\langle v, \int_{0}^{t} \alpha(s) \, ds \right\rangle \\
    &= \int_{0}^{t} \langle v, \alpha(s) \rangle \, ds\\
    &\leq \int_{0}^{t} \Vert v \Vert_2 \cdot \Vert \alpha(s) \Vert_2 \, ds
  \end{align*}
  nach komponentenweiser Integration und einer Anwendung
  der Cauchy-Schwarz Ungleichung. Teilung durch $\Vert v \Vert_2$ liefert
  \[
    \Vert v \Vert_2 \leq \int_{0}^{t} \Vert \alpha(s)\Vert_2 \, ds.
    \qedhere
  \]
\end{proof}

\begin{proof}[Beweis von Cauchy-Lipschitz-Picard-Lindelöf]
  Wir betrachten nur den Fall, dass $U = \mathbb{R}^n$ gilt.
  Allgemeine $U$ zu betrachten benötigt nicht viele zusätzliche
  Überlegungen.
  Sei $T > 0$.
  Betrachte $V = C([-T, T], \mathbb{R}^n)$
  mit der Norm
  \[
    \Vert \alpha \Vert_{\infty} = \max \left\{\Vert \alpha(t) \Vert_2
    \mid t \in [-T, T]\right\}.
  \]
  Definiere nun
  \begin{align*}
    P \colon V & \to V \\
    \alpha & \mapsto P(\alpha)
  \end{align*}
  durch
  \[
    P(\alpha)(t) = p + \int_{0}^{t} X(\alpha(s)) \, ds
  \]
  für alle $t \in [-T, T]$.
  Wir bemerken, dass $X \circ \alpha$ stetig ist,
  da $\alpha$ und $X$ beide stetig sind. Deshalb
  ist $P(\alpha)$ auf $(-T, T)$ differenzierbar.
  Weiterhin haben wir bereits gesehen,
  dass für $t \in (-T, T)$ gilt, dass
  \[
    \frac{d}{dt} P(\alpha)(t) = X(\alpha(t)).
  \]
  Nimm nun an, dass $P$ einen Fixpunkt $\gamma \in V$
  hat
  (wir schreiben $\gamma$ für diesen Punkt,
  da die Punkte in $V$ Kurven sind).
  Dann gilt für alle $t \in (-T, T)$, dass
  \[
    \dot \gamma(t) = \frac{d}{dt}\gamma(t) =
    \frac{d}{dt} P(\gamma)(t)
    = X(\gamma(t)).
  \]
  Ausserdem ist
  \[
    \gamma(0) = P(\gamma)(0) = p + \int_{0}^{0} X(\gamma(s)) \, ds
    = p,
  \]
  also ist $\gamma$ eine Lösungskurve unserer
  Differentialgleichung mit $\gamma(0) = p$.
  Umgekehrt, sei $\gamma \in V$ so, dass
  $\gamma|_{(-T, T)}$ eine Lösungskurve der Differentialgleichung
  $\dot \gamma(t) = X(\gamma(t))$ mit $\gamma(0) = p$ ist.
  Dann gilt für alle $t \in (-T, T)$,
  dass
  \[
    P(\gamma(t)) = p + \int_{0}^{t} X(\gamma(s)) \, ds
    = p + \int_{0}^{t} \dot \gamma(s) \, ds
    = p + \gamma(t) - \gamma(0)
    = \gamma(t).
  \]
  Aus der Stetigkeit von $\gamma$ und $P(\gamma)$
  folgt auch $P(\gamma)(\pm T) = \gamma(\pm T)$.
  Also ist $\gamma$ ein Fixpunkt von $P$.
  Zusammengefasst erhalten wir, dass
  die Fixpunkte von $P$ genau die Lösungskurven
  der Differentialgleichung sind.

  Um Banachs Fixpunktsatz anzuwenden, zeigen wir nun,
  dass für alle $T < 1/k$ die Abbildung $P$ kontrahierend ist.
  Dazu sei $t \in [-T, T]$ beliebig,
  und $\alpha, \beta \in V$. Schätze ab, dass
  \begin{align*}
    \Vert P(\alpha(t)) - P(\beta(t)) \Vert_2
    & = \left\Vert \int_{0}^{t} X(\alpha(s)) - X(\beta(s)) \, ds
    \right\Vert_2\\
    &\leq \int_{0}^{t} \Vert X(\alpha(s)) - X(\beta(s)) \Vert_2 \, ds\\
    &\leq k \int_{0}^{t} \Vert \alpha(s) - \beta(s) \Vert_2 \, ds \\
    &\leq k \cdot T \cdot \Vert \alpha - \beta \Vert_{\infty}.
  \end{align*}
  Wir schliessen, dass $\Vert P(\alpha) - P(\beta) \Vert_{\infty}
  \leq k \cdot T \cdot \Vert \alpha - \beta \Vert_{\infty}$,
  also $P$ kontrahierend ist.
  Wir schliessen aus der Vollständigkeit von $V$
  mit Hilfe des Fixpunktsatzes von Banach, dass $P$
  einen eindeutigen Fixpunkt $\gamma \in V$ hat.
  Also ist $\gamma \in V$ die eindeutige Lösung
  zur Differentialgleichung $\dot \gamma(t) = X(\gamma(t))$
  mit $\gamma(0) = p$.
\end{proof}

\section{Lineare Differentialgleichungen}
Sei $A \colon \mathbb{R}^n \to \mathbb{R}^n$ linear.
Aus Abschnitt~\ref{sec:continuity}
wissen wir, dass die Operatornorm
$\Vert A \Vert_{\text{op}}$ endlich ist.
Insbesondere ist $A$ Lipschitz-stetig mit Konstante
$k = \Vert A \Vert_{\text{op}}$.
Tatsächlich gilt für alle $v, w \in \mathbb{R}^n$, dass
\[
  \Vert A(w) - A(v) \Vert_2 \leq \Vert A \Vert_{\text{op}} \cdot
  \Vert v - w \Vert_2.
\]
Wir folgern, dass die Differentialgleichung
$\dot \gamma(t) = A(\gamma(t))$ zu jeder
Anfangsbedingung $p \in \mathbb{R}^n$
und jeder Zeit $T < 1/k$
eine eindeutige Lösungskurve
$\gamma \colon (-T, T) \to \mathbb{R}^n$ hat.
Sei $V = C([-T, T], \mathbb{R}^n)$.
Wir hatten $\gamma$ als Fixpunkt eines kontrahierenden
Operators
\begin{align*}
  P \colon V & \to V \\
\end{align*}
konstruiert, wobei
\[
  P(\alpha)(t) = p + \int_{0}^{t} A(\alpha(s)) \, ds.
\]
Sei $\gamma_0 \in V$ beliebig. Definiere rekursiv
$\gamma_{n+1} = P(\gamma_n) \in V$.
Dann gilt $\gamma = \lim_{n \to \infty} \gamma_n$.
Wähle nun $\gamma_0(t) = p$ konstant.
Wir berechnen nun einige Glieder der Folge
${(\gamma_{n})}_{n \in \mathbb{N}}$.
Das erste Glied ist
\begin{align*}
  \gamma_1(t)
  & = P(\gamma_0)(t) \\
  &= p + \int_{0}^{t} A(\gamma_0(s)) \, ds \\
  &= p + t \cdot A(p).
\end{align*}
Mit der Linearität von $A$ berechnen wir
\begin{align*}
  \gamma_2(t)
  &= p + \int_{0}^{t} A(\gamma_1(s)) \, ds  \\
  &= p + \int_{0}^{t} A(p + s \cdot A(p)) \, ds \\
  &= p + \int_{0}^{t} A(p) + s \cdot A^2(p) \, ds \\
  &= p + t \cdot A(p) + \frac{t^2}{2}A^2(p).
\end{align*}
Um unsere Vermutung, wie diese Folge weitergeht,
zu bestätigen, berechnen wir noch einen letzten
Term explizit:
\begin{align*}
  \gamma_3(t)
  & = P(\gamma_2)(t)\\
  &= p + \int_{0}^{t} A(p + s \cdot A(p) + s^2 /2 \cdot A^2(p)) \, ds \\
  &= p + t \cdot A(p) + \frac{t^2}{2} A^2(p) + \frac{t^3}{3!} A^3(p).
\end{align*}
Induktiv erhalten wir für alle $n \in \mathbb{N}$, dass
\[
  \gamma_n(t)
  = \sum_{k=0}^{n} \frac{t^k}{k!} A^k(p)
\]
(mit der Konvention, dass $A^0$ die Identitätsabbildung ist).

\begin{definition}
  Sei $A \colon \mathbb{R}^n \to \mathbb{R}^n$ linear.
  Die \emph{Endomorphismenexponentialabbildung} ist
  \[
    \exp(tA) = \sum_{k=0}^{\infty} \frac{t^k}{k!} A^k.
  \]
\end{definition}

\begin{remark}
  Aus den Beweisen der Theoreme von Banach
  und Cauchy-Lipschitz-Picard-Lindelöf wissen wir bloss,
  dass dieser Grenzwert für alle $|t| < 1/\Vert A \Vert_{\text{op}}$
  existiert. Tatsächlich existiert der Grenzwert für alle
  $t \in \mathbb{R}$, unabhängig von $A$.
  Benutze dazu, dass für die Operatornorm die Ungleichung
  $\Vert A^k \Vert_{\text{op}} \leq \Vert A \Vert_{\text{op}}^k$
  gilt. Dann folgt nämlich
  $\Vert e^{tA} \Vert_{\text{op}} \leq e^{|t| \cdot \Vert A \Vert_{\text{op}}}$.
  Also ist die Abbildung $\exp(tA)$ ein Endomorphismus von $\mathbb{R}^n$
  für alle $t \in \mathbb{R}$.
\end{remark}

Wir schliessen, dass die Lösung zur Differentialgleichung
$\dot \gamma(t) = A(\gamma(t))$ mit $\gamma(0) = p$
durch
\[
  \gamma(t) = e^{tA} \cdot p
\]
gegeben ist.

\begin{examples}
  Sei $A \colon \mathbb{R}^2 \to \mathbb{R}^2$ linear,
  repräsentiert bezüglich der Standardbasis
  durch eine Matrix
  \[
    A =
    \begin{pmatrix}
      a & b \\ c & d
    \end{pmatrix}.
  \]
  \begin{enumerate}[(1)]
    \item Betrachte
      \[
        A =
        \begin{pmatrix}
          a & 0 \\ 0 & d
        \end{pmatrix} = \text{diag}(a, d).
      \]
      Berechne für $k \in \mathbb{N}$, dass
      \(
        A^k =
        \text{diag}(a^k, d^k)
      \),
      Also ist
      \(
        e^{tA} =
        \text{diag}(
        e^{ta}, e^{td})
      \).
      Wir betrachten den Spezialfall $a = 1, d = -1$.
      Dann ist
      \(
        e^{tA} =
        \text{diag}(
          e^t, e^{-t}
          )
      \).
      Einige Lösungskurven sind in
      Tabelle~\ref{tab:1-1}
      aufgelistet und in Abbildung~\ref{fig:1-1}
      gezeichnet.

      \begin{table}[htb]
        \center
        \begin{tabular}{c|c}
          Anfangsbedingung $p \in \mathbb{R}^2$ & Lösungskurve $\gamma(t)$\\
          \hline
          $(\pm 1, 0)$ & $( \pm e^t, 0)$ \\
          $(0, \pm 1)$ & $(0, \pm e^{-t})$ \\
          $\pm(1, 1)$ & $\pm(e^t, e^{-t})$ \\
          $\pm (1, -1)$ & $\pm (e^t, -e^{-t})$
        \end{tabular}
        \caption{Einige Lösungen der Differentialgleichung
        zur Matrix $A = \text{diag}(1, -1)$.}%
        \label{tab:1-1}
      \end{table}

      \begin{figure}[htb]
        \centering
        \includegraphics{figures/1-1}
        \caption{Einige Lösungen der Differentialgleichung
        zur Matrix $A = \text{diag}(1, -1)$.}%
        \label{fig:1-1}
      \end{figure}
    \item Betrachte die Matrix
      \[
          A =
          \begin{pmatrix}
            0 & -1 \\ 0 & 0
          \end{pmatrix}
      \]
      und bemerke, dass $A^2 = 0$ gilt.
      Folglich ist
      \[
        e^{tA} =
        \begin{pmatrix}
          1 & -t \\ 0 & 1
        \end{pmatrix}.
      \]
      Die Lösungskurven von $A$
      sind konstant auf der $x$-Achse,
      und ausserhalb sind sie parallel dazu,
      wobei ihre Geschwindigkeit mit zunehmender
      $y$-Komponente linear wächst.
    \item Die Matrix
      \[
        A =
        \begin{pmatrix}
          0 & 0 \\ 1 & 0
        \end{pmatrix}
      \]
      liefert ein rotiertes Bild des obigen Beispiels.
    \item Die Matrix
      \[
        A =
        \begin{pmatrix}
          0 & -1 \\ 1 & 0
        \end{pmatrix}
      \]
      ist eine Drehung um den Winkel $\pi/2$.
      Die Potenzen von $A$ sind
      $A^2 = -1$, $A^3 = -A$, und $A^4 = 1$.
      Dann geht es periodisch weiter.
      Berechne
      \[
        e^{tA} =
        \begin{pmatrix}
          1 - t^2/2! + t^4/4! -  \cdots
          & -t + t^3/3! - t^5/5! + \cdots \\
          t - t^3/3! + t^5/5! - \cdots
          &
          1 - t^2/2! + t^4/4! -  \cdots
        \end{pmatrix}
        =
        \begin{pmatrix}
          \cos(t) & -\sin(t) \\
          \sin(t) & \cos(t)
        \end{pmatrix}.
      \]
      Wir haben also nun hergeleitet, dass
      die Lösungskurven dieses Vektorfelds konzentrische
      Kreise sind.
      Wir sehen auch, dass im allgemeinen
      $e^{t(A + B)} = e^{tA} \cdot e^{tB}$ nicht gilt
      (das gilt nur, wenn $AB = BA$).
  \end{enumerate}
\end{examples}

\begin{remark}
Die Matrix $e^{tA}$ lässt sich für diagonalisierbare
Matrizen $A \in \mathbb{R}^{n \times n}$ relativ einfach
berechnen. Sei $A$ eine solche Matrix, das heisst,
es existiert $P \in \mathbb{R}^{n \times n}$
mit
$\det(P) \neq 0$ und $PAP^{-1} = D = \text{diag}(\lambda_1, \dots, \lambda_n)$
ist diagonal.
Es gilt dann
\[
A^k = {(P^{-1}DP)}^k = P^{-1}D^k P,
\]
also folgt $e^{tA} = P^{-1}e^{tD}P$, wobei
$e^{tD} = \text{diag}(e^{t\lambda_1}, \dots, e^{t \lambda_n})$.
Dies funktioniert genau so, falls $A$ über $\mathbb{C}$
diagonalisierbar ist.
Für nicht diagonalisierbare $A$ wird die Jordan--Chevalley-Zerlegung
zur Berechnung von $e^{tA}$ verwendet.
Schreibe dazu $A = D + N$ für eine diagonalisierbare
Matrix $D$ und eine nilpotente Matrix $N$ mit
$DN = ND$.
\end{remark}

\section{Differentialgleichungen in einer Variable}
Sei $f \colon \mathbb{R} \to \mathbb{R}$ stetig
(aber nicht unbedingt linear).
Wir erhalten eine Differentialgleichung
für $y \colon (-T, T) \to \mathbb{R}$
mit $y(0) = y_0 \in \mathbb{R}$,
nämlich $\dot y = f(y(t))$.
Falls $f(y_0) = 0$, dann ist
$y(t) = y_0$ eine konstante Lösung.
Falls $f(y_0) \neq 0$, schreibe (lokal) um, dass
\[
  \dot y(t) \frac{1}{f(y(t))} = 1.
\]

\begin{lemma*}
  Mit den Voraussetzungen oben,
  sei $g \colon \mathbb{R} \to \mathbb{R}$ eine Stammfunktion
  von $1/f$, das heisst $g'(x) = 1/f(x)$.
  Dann gilt $g(y(t)) = t + c$.
\end{lemma*}

\begin{proof}
  Berechne
  \begin{align*}
    \frac{d}{dt} \, g(y(t))
    &= g'(y(t)) \cdot \frac{d}{dt} \, y(t)\\
    &= \frac{1}{f(y(t))} \dot y(t) \\
    &= 1. && \qedhere
  \end{align*}
\end{proof}

Mit etwas Glück lässt sich diese Formel nach $y(t)$ auflösen.

\begin{examples}
  \leavevmode
  \begin{enumerate}[(1)]
    \item Betrachte $f(x) = x$, das heisst, wir untersuchen
      die Differentialgleichung $\dot y(t) = y(t)$.
      Sei $y_0 > 0$ und $g(x) = \log(x)$.
      Dann sind die Voraussetzungen vom Lemma erfüllt.
      Es gilt also $\log(y(t)) = t + c$, also
      $y(t) = e^{t + c} = e^t \cdot e^c$.
      Mit $y(0) = e^c = y_0$ folgt $y(t) = y_0 e^t$.
    \item Sei $f(x) = x^2$, das heisst
      wir untersuchen $\dot y(t) = {(y(t))}^2$.
      Sei $y_0 > 0$ und
      $g(x) = -1/x$.
      Aus $-1/y(t) = t + c$ erhalten wir
      $y(t) = -1/(t + c)$.
      Mit  $y(0) = -1/c = y_0$ erhalten wir
      \[
        y(t) = \frac{1}{1/y_0 - t}.
      \]
      Im Spezialfall $y_0 = 1$ sehen wir,
      dass die Lösung $y(t) = \frac{1}{1-t}$
      in endlicher Zeit divergiert: es gilt
      \[
        \lim_{t \to 1} \frac{1}{1-t} = +\infty.
      \]
    \item Sei $f(x) = 2\sqrt x$. Dann ist
      $\dot y (t) = 2 \sqrt{y(t)}$.
      Wir haben $g(x) = \sqrt x$,
      also ist $\sqrt{y(t)} = t + c$.
      Wir schliessen, dass
      $y(t) = {(t + c)}^2$ eine Lösung
      ist.
      Mit $y(0) = y_0$ folgt
      $y(t) = {(t + \sqrt {y_0})}^2$.
      Im Grenzfall  $y_0 = 0$
      erhalten wir $y(t) = t^2$.
      Es gibt aber noch eine alternative
      Lösung zur selben Anfangsbedingung,
      nämlich $y(t) = 0$.
      Diese Differentialgleichung modelliert das
      Abflussverhalten einer Badewanne.
      Dies kann einen physikalischen Hinweis darauf geben, dass
      die Lösungen nicht eindeutig sind. Tatsächlich
      weiss man, wenn man eine leere Badewanne antrifft
      nicht, wann sie zum letzten Mal voll war.
  \end{enumerate}
\end{examples}

\begin{remarks}
   \leavevmode
   \begin{enumerate}[(i)]
   \item Die Lösungskurven brauchen nicht für alle Zeiten zu existieren.
     Im Beispiel zwei zum Beispiel galt $\lim_{t \to 1} y(t) = \infty$.
   \item Die Lösungskurven brauchen bei gegebener Anfangsbedingung nicht
     eindeutig zu sein, siehe Beispiel (3). Für die
     Eindeutigkeit ist die Lipschitz-Stetigkeit des
     Vektorfelds essentiell.
   \item Für die Existenz von Lösungen reicht
     die Stetigkeit des Vektorfelds. Diese Tatsache
     ist als ``Satz von Peano'' bekannt.
 \end{enumerate}
\end{remarks}

Seien $a_0, a_1, \dots, a_{n-1} \in \mathbb{R}$.
Wir möchten die
\emph{lineare homogene Differentialgleichung $n$-ter Ordnung}
\[
  y^{(n)}(t)
  + a_{n-1}y^{(n-1)}(t)
  + \cdots
  + a_2 y''(t)
  + a_1 y'(t)
  + a_0 y(t)
  = 0
\]
mit Anfangsbedingungen $y(0) = a_0, y'(0) = y_1, \dots
y^{(n-1)}(0) = y_{n-1}$.
Konkret gesucht ist eine $n$-mal differenzierbare
Lösungsfunktion $y \colon \mathbb{R} \to \mathbb{R}$,
die diese Bedingungen erfüllt.
Unsere Methode ist es, die Differentialgleichung auf eine
lineare Differentialgleichung erster Ordnung auf $\mathbb{R}^n$
zurückzuführen.
Führe dazu Koordinaten
$y, y', y'', \dots, y^{(n-1)}$ auf $\mathbb{R}^n$ ein.
Schreibe
$\gamma(t) = (y(t), y'(t), \dots, y^{(n-1)}(t))$,
\[
  A =
  \begin{pmatrix}
    0 & 1 & 0 & \cdots & 0 \\
    0 & 0 & 1 & \cdots & 0 \\
      &   & \ddots & & \vdots \\
      & & &  0 & 1 \\
    -a_0 & -a_1 & -a_2 & \cdots & -a_{n-1}
  \end{pmatrix}.
\]
Dann liest sich unsere Differentialgleichung als
$\dot \gamma(t) = A \cdot \gamma(t)$.
Also ist jede lineare homogene Differentialgleichung
$n-ter$ Ordnung auf $\mathbb{R}$ äquivalent zu einer linearen
homogonen Differentialgleichung erster Ordnung auf $\mathbb{R}^n$.
Die Anfangsbedingung in unserer Formulierung lautet
$\gamma(0) = (y_0, y_1, \dots, y_{n-1})$.
Die bekannte Lösung $\gamma(t) = e^{tA} \cdot \gamma(0)$
liefert die gewünschte Lösung $y(t)$ durch herauslesen
der ersten Komponente von $\gamma$.

\begin{examples}
  \leavevmode
  \begin{enumerate}[(1)]
    \item Die Differentialgleichung $\ddot y(t) + y(t) = 0$
      ist als
      \emph{freie Schwingung}
      bekannt. In der Physik ist $\ddot y = -g/\ell \cdot y$,
      wobei $g$ die Gravitationsbeschleunigung und $l$
      die Länge des Pendels ist.
      Die Funktion $y$ beschreibt den Auslenkungswinkel.
      Wir betrachten den Fall $g/\ell = 1$.
      Setze $\gamma(t) = (y(t), \dot y(t))$.
      Dann ist
      \[
        \dot \gamma(t) =
        \begin{pmatrix}
          \dot y(t) \\
          \ddot y(t)
        \end{pmatrix}
        =
        \begin{pmatrix}
          0 & 1 \\ -1 & 0
        \end{pmatrix}
        \begin{pmatrix}
          y(t) \\
          \dot y(t)
        \end{pmatrix}.
      \]
      Wenn wir die Anfangbedingungen $y_0 = 0$
      und $y_1 = 1$ betrachten, dann ist die Lösung
      \[
        \gamma(t) =
        \begin{pmatrix}
          \cos t & \sin t \\
          -\sin t & \cos t
        \end{pmatrix}
        \begin{pmatrix}
          0 \\ 1
        \end{pmatrix}
        =
        \begin{pmatrix}
          \sin t\\
          \cos t
        \end{pmatrix}
      \]
    \item Die Differentialgleichung
      $\ddot y(t) + \dot y(t) + y(t) = 0$
      ist als \emph{gedämpfte Schwingung} bekannt.
      In der Physik trifft man diese Gleichung
      als $\ddot y(t) = -ky(t) - \theta \dot y (t)$ an,
      wobei $\theta$ ein Reibungsterm ist.
      Setze $\gamma(t) = (y(t), \dot y(t))$ wie oben.
      Wir erhalten die Differentialgleichung
      \[
        \dot \gamma(t) =
        \begin{pmatrix}
          0 & 1 \\
          -1 & -1
        \end{pmatrix}
        \begin{pmatrix}
          y(t) \\
          \dot y(t)
        \end{pmatrix}.
      \]
      Um die explizite Lösung $e^{tA}$ für
      \[
        A =
        \begin{pmatrix}
          0 & 1 \\
          -1 & -1
        \end{pmatrix}
      \]
      zu bestimmen,
      berechne das charakteristische
      Polynom $\chi_{A}(t) = t^2 + t + 1$
      und dessen Eigenwerte und Eigenvektoren
      $v_1$ und $v_2$
      um $A$ zu diagonalisieren.
      Vergleiche Serie 4.
  \end{enumerate}
\end{examples}

\section{Rektifizierbare Kurven}
Das Material in diesem Abschnitt ist im
Abschnitt~177 in~\cite{heuser} nachzuschlagen.
\begin{definition}
  Eine Kurve $\gamma \colon [0, T] \to \mathbb{R}^n$ heisst
  \emph{stetig differenzierbar}, falls ein offenes Intervall
  $(a, b)$ existiert, welches $[0, T]$ enthält,
  und eine stetig differenzierbare
  Kurve $\overline \gamma \colon (a, b)
  \to \mathbb{R}^n$
  mit $\overline \gamma |_{[0, T]} = \gamma$
  existiert.
  Wir setzen
  \[
    L(\gamma) = \int_{0}^{T} \Vert \dot \gamma
    (s) \Vert_2 \, ds.
  \]
\end{definition}

\begin{example}
  Betrachte die Kurve
  \begin{align*}
    \gamma \colon [0, 2\pi] & \to \mathbb{R}^2 \\
    t & \mapsto (\cos(t), \sin(t)).
  \end{align*}
  Berechne
  \(
  \dot \gamma(s) = (- \sin(s), \cos(s))
  \)
  und $\Vert \dot \gamma(s) \Vert_2 = 1$
  für alle $s \in \mathbb{R}$.
  Wir folgern, dass
  \[
    L(\gamma) = \int_{0}^{2\pi} 1 \, ds
    = 2\pi.
  \]
\end{example}

Eine Kurve braucht nicht stetig differenzierbar
zu sein, um eine endliche ``Länge'' zu haben.

\begin{examples}
  Folgende Beispiele sollen zeigen, dass
  unser $L(\gamma)$ als Längenbegriff nicht
  allgemein genug ist.
  \begin{enumerate}[(1)]
    \item Betrachte die Kurve
      \begin{align*}
        \gamma \colon [0, 1] & \to \mathbb{R}^2 \\
        t & \mapsto (t, \sqrt{t}).
      \end{align*}
      Dann ist $\gamma$
      im Nullpunkt nicht stetig
      differenzierbar: es gilt
      \[
        \lim_{s \to 0}
        \Vert \dot \gamma(s) \Vert_2
        = \lim_{s \to 0}
        \sqrt{1 + 1/(4s)} = +\infty.
      \]
      Somit ist $\dot \gamma$
      nicht Riemann-integrierbar.
      Dennoch hat $\gamma$ eine
      endliche ``Länge'' (siehe auch
      Abbildung~\ref{fig:rectifiability}).
      Der Grund dazu ist folgender.
      Sei $\varepsilon > 0$.
      Dann gilt
      \[
        \int_{\varepsilon}^{1}
        \Vert \dot \gamma(s) \Vert_2\, ds
        =
        \int_{\varepsilon}^{1}
        \sqrt{1 + 1/(4s)} \, ds
        \leq \int_{\varepsilon}^{1}
        \sqrt{5/(4s)} \, ds
        \leq \sqrt 5 (1 - \sqrt{\varepsilon})
        \leq \sqrt 5.
      \]
    \item Betrachte die Kurve $\gamma \colon [0, 1] \to \mathbb{R}^2$
      aus Abbildung~\ref{fig:rectifiability},
      welche durch stückweise gerade Abschnitte,
      deren Länge durch Terme einer geometrischen
      Reihe $\sum {(1/2)}^n$ gegeben sind, definiert ist.
      Wir geben keine expliziten Formeln dafür an.
      Die Kurve $\gamma$ ist in abzählbar vielen
      Punkten nicht differenzierbar,
      hat aber ``Länge'' $1$.
    \item Es gibt sogar Kurven endlicher ``Länge'',
      welche in überabzählbar vielen Punkten nicht
      differenzierbar sind.
      Eine Konstruktion inspiriert durch
      die ``Kochsche Schneeflocke'' liefert
      solch eine Kurve.
  \end{enumerate}
\end{examples}

\begin{figure}[htb]
  \centering
  \begin{minipage}{0.50\textwidth}
    \centering
    \includegraphics{figures/sqrt}
  \end{minipage}%
  \begin{minipage}{0.50\textwidth}
    \centering
    \includegraphics{figures/spiral}
  \end{minipage}%
  \caption{Die Kurven aus den Beispielen (1) und (2)}%
  \label{fig:rectifiability}
\end{figure}

\begin{definition}
  Sei $\gamma \colon [0, 1] \to \mathbb{R}^n$ stetig.
  Eine \emph{Partition} des Intervalls
  $[0, 1]$ ist eine endliche Folge
  von Zeitparametern $0 = t_0 < t_1 < \cdots < t_N = 1$.
  Wir notieren diese Partition als
  $P = \{t_0, t_1, \dots, t_N\}$.
  Setze
  \[
    L(\gamma ; P) =
    \sum_{k=1}^{N}  \Vert \gamma(t_k) - \gamma(t_{k-1}) \Vert_2.
  \]
  Dies ist in Abbildung~\ref{fig:def-rectifiability} veranschaulicht.
\end{definition}

\begin{figure}[htb]
    \centering
    \incfig[0.5]{def-rectifiability}
    \caption{Eine Partition der Länge $N = 3$}%
    \label{fig:def-rectifiability}
\end{figure}

\begin{remark}
  Falls eine Partition $P_2$ feiner ist als $P_1$,
  das heisst $P_1 \subset P_2$,
  dann gilt $L(\gamma ; P_1) \leq L(\gamma ; P_2)$.
  Der Grund dafür ist die Dreiecksungleichung.
\end{remark}

\begin{definition}
  Sei $\gamma \colon [0, 1] \to \mathbb{R}^n$ stetig.
  Dann ist die \emph{Länge} von $\gamma$
  das Supremum von $L(\gamma ; P)$, wobei
  $P$ über alle Partitionen von $[0, 1]$ läuft.
  Falls die Länge von $\gamma$ endlich ist,
  dann heisst $\gamma$ \emph{rektifizierbar}.
\end{definition}

\begin{examples}
  \leavevmode
  \begin{enumerate}[(1)]
    \item Sei $\gamma \colon [0, 1] \to \mathbb{R}^n$
      Lipschitz-stetig
      mit Konstante $k \geq 0$.
      Dann gilt für alle Zeiten $t_{k-1}, t_k \in [0, 1]$, dass
      \(
        \Vert \gamma(t_k) - \gamma(t_{k-1})\Vert_{2}
        \leq k \cdot | t_k - t_{k-1} |
      \).
      Für jede Partition $P$ von $[0, 1]$ gilt also
      \[
        L(\gamma ; P) = \sum_{k=1}^{N} \Vert \gamma(t_k) - \gamma(t_{k-1})
        \Vert_2 \leq k.
      \]
      Also ist $\gamma$ rektifizierbar mit Länge höchstens $k$.
    \item Betrachte die Kurve
      \begin{align*}
        \gamma \colon [0, 1] & \to \mathbb{R}^2 \\
        t & \mapsto
        \begin{cases}
          (0, 0) & t = 0, \\
          (t, t \cos(\pi/t)) & t > 0.
        \end{cases}
      \end{align*}
      Betrachte die Partition
      $P = \{0, 1/N, 1/(N-1), \dots, 1/2, 1\}$ für festes
      $N \in \mathbb{N}$.
      Dann ist $L(\gamma ; P) \geq 1 + 1/2 + \cdots + 1/N$.
      Vergleiche dazu Serie 5.
      Es folgt, dass die Länge von $\gamma$ nicht endlich ist,
      da die harmonische Reihe divergiert.
  \end{enumerate}
\end{examples}

\begin{theorem*}
  Sei $\gamma \colon [0, 1] \to \mathbb{R}^n$ stetig differenzierbar.
  Dann ist $\gamma$ rektifizierbar, und die Länge von $\gamma$ ist
  \[
    L(\gamma) = \int_{0}^{1} \Vert \dot \gamma(s) \Vert_2 \, ds.
  \]
\end{theorem*}

\begin{proof}
  Wir zeigen zunächst, dass die Länge von $\gamma$
  höchstens $L(\gamma)$ ist.
  Sei dazu $P = \{t_0, t_1, \dots, t_N\}$ eine Partition
  von $[0,1]$.
  Für alle $a,b \in [0, 1]$ mit $a \leq b$ gilt
  \[
    \gamma(b) - \gamma(a) = \int_{a}^{b} \dot \gamma(s) \, ds.
  \]
  Es folgt, dass
  \[
    \Vert \gamma(b) - \gamma(a) \Vert_2 =
    \left\Vert \int_{a}^{b} \dot \gamma(s) \, ds \right\Vert_2
    \leq \int_{a}^{b} \Vert \dot \gamma(s) \Vert_2 \, ds.
  \]
  Siehe dazu die Integralabschätzung in der Vorbereitung
  zum Beweis von Theorem von Cauchy-Lipschitz-Picard-Lindelöf.
  Wir folgern, dass
  \[
    L(\gamma ; P) = \sum_{k=1}^{N}
    \Vert \gamma(t_k) - \gamma(t_{k-1}) \Vert_2
    \leq
    \sum_{k=1}^{N} \int_{t_{k-1}}^{t_k} \Vert \dot \gamma(s) \Vert_2 \, ds
    = \int_{0}^{1} \Vert \dot \gamma(s) \Vert_2 \, ds = L(\gamma).
  \]
  Die Länge von $\gamma$ ist also höchstens $L(\gamma) < \infty$,
  und $\gamma$ ist rektifizierbar.

  Als nächstes ist die Gleichheit zu zeigen.
  Sei dazu $\varepsilon > 0$.
  Wir konstruieren eine Partition
  $P$ mit $L(\gamma ; P) \geq L(\gamma) - 2\varepsilon$.
  Daraus wird folgen, dass das Supremum über alle Partitionen
  genau $L(\gamma)$ ist.
  Da $\dot \gamma \colon [0, 1] \to \mathbb{R}^n$ stetig ist
  und $[0, 1]$ kompakt ist, folgt, dass $\dot \gamma$
  gleichmässig stetig ist.
  Es existiert also $\delta > 0$ so, dass
  für alle $a, b \in [0, 1]$ mit $|b - a| \leq \delta$
  gilt, dass
  $\Vert \dot \gamma(b) - \dot \gamma(a) \Vert_2 \leq \varepsilon$.
  Wähle nun $N \in \mathbb{N}$ mit $1/N \leq \delta$.
  Betrachte die Partition $P = \{0, 1/N, 2/N, \dots, (N-1)/N, 1\}$.
  Sei $k \in \{1, \dots, N\}$ beliebig.
  Dann gilt für $s \in [t_{k-1}, t_k]$, dass
  \[
  \Vert \dot \gamma(s) \Vert_2 \leq \Vert \dot \gamma
  (t_{k-1}) \Vert_2
  + \Vert \dot \gamma (s) - \dot \gamma(t_{k-1}) \Vert_2
  \leq \Vert \dot \gamma(t_{k-1}) \Vert_2 + \varepsilon,
  \]
  da $|s - t_{k-1}| \leq 1/N$.
  Es gilt also, dass
  \begin{align*}
       \int_{t_{k-1}}^{t_k} \Vert \dot \gamma (s) \Vert_2 \, ds
       &\leq
    \int_{t_{k-1}}^{t_k} \Vert \dot \gamma(t_{k-1}) \Vert_2 \, ds
    + \int_{t_{k-1}}^{t_k} \varepsilon \, ds \\
       &\leq 1/N \cdot \Vert \dot \gamma(t_{k-1}) \Vert_2 + \varepsilon/N.
  \end{align*}
  Berechne weiterhin, dass
  \begin{align*}
    \left\Vert
    \int_{t_{k-1}}^{t_k} \dot \gamma(s) \, ds
    \right\Vert_2
    &=
    \left\Vert
    \int_{t_{k-1}}^{t_k} \dot \gamma(t_{k-1}) \, ds
    + \int_{t_{k-1}}^{t_k} \dot \gamma(s) - \dot \gamma(t_{k-1}) \, ds
    \right\Vert_2 \\
    &\geq
    \left\Vert
    \int_{t_{k-1}}^{t_k} \dot \gamma(t_{k-1}) \, ds
    \right\Vert_2
    -
    \left\Vert
    \int_{t_{k-1}}^{t_k} \dot \gamma(s) - \dot \gamma(t_{k-1}) \, ds
    \right\Vert_2 \\
    &\geq
    \frac{1}{N} \cdot \Vert \dot \gamma(t_{k-1}) \Vert_2
    - \frac{\varepsilon}{N}.
  \end{align*}
  Es folgt, dass
  \begin{align*}
    \Vert \gamma(t_k) - \gamma(t_{k-1}) \Vert_2
    & \geq \frac{\Vert \dot \gamma(t_{k-1}) \Vert_2}{N}
    - \frac{\varepsilon}{N}\\
    &\geq \int_{t_{k-1}}^{t_k} \Vert \dot \gamma(s) \Vert_2 \, ds
    - \frac{2\varepsilon}{N}.
  \end{align*}
  Aufsummieren liefert, dass
  \[
    L(\gamma ; P) \geq \int_{0}^{1} \Vert \dot \gamma(s) \Vert_2 \, ds
    - 2 \varepsilon. \qedhere
  \]
\end{proof}

\begin{example}[Schwarzscher Stiefel]
  Wir versuchen, den Inhalt einer Fläche durch eine
  ``feine'' Triangulierung zu approximieren.
  Leider geht der naive Ansatz schief!
  Dazu untersuchen wir
  einen Zylinder der Höhe $h$ mit Radius $1$.
  Wähle $N \in \mathbb{N}$ gross,
  und Punkte $P_1, \dots, P_N$ auf der Höhe $0$,
  welche ein regelmässiges $N$-Eck bilden.
  Wähle versetzte Punkte $Q_1, \dots, Q_N$ auf der Höhe $h$,
  welche auch ein regelmässiges $N$-Eck bilden.
  Siehe Abbildung~\ref{fig:n-gons}.
  Wir erhalten eine Triangulierung
  mit $2N$ Dreiecken.

\begin{figure}[htb]
  \centering
  \begin{minipage}{0.50\textwidth}
    \centering
    \includegraphics{figures/n-gons}
  \end{minipage}%
  \begin{minipage}{0.50\textwidth}
    \centering
    \includegraphics{figures/triangulation}
  \end{minipage}%
   \caption{Die Punkte $P_i$ und $Q_i$ von oben betrachtet
   und die daraus resultierende Triangulierung}%
   \label{fig:n-gons}
\end{figure}

  Sei $\Delta$ eines der Dreiecke.
  Die Fläche von $\Delta$ ist ungefähr
  $\pi / N \cdot 1$.
  Die Gesamtfläche ist also ungefähr $2 \pi$,
  was genau die Fläche des Zylinders ist.
  Sei aber nun $h$ sehr klein,
  zum Beispiel $1/N^3$.
  Dann ist die Höhe von $\Delta$ ungefähr
  proportional $1/N^2$ (und nicht
  wie erwartet $1/N^3)$.
  Das liegt daran, dass
  $\cos (1/N)$ etwa  $1 - 1/(2N^2)$ ist.
  Die Gesamtfläche ist dann etwa
  proportional zu
  $1/N^2$.
  Teilen wir nun die ursprüngliche Fläche
  in $N^3$ Unterzylinder,
  so ist der Gesamtflächeninhalt
  der Dreiecke
  proportional zu $N^3/N^2 = N$.
  Unsere Definition der Rektifizierbarkeit ist
  somit für Flächen unangebracht. Ein besserer
  Flächenbegriff wird in der Differentialgeometrie studiert.
\end{example}



\end{document}
